{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Alyona's personal blog","text":"<p>Hi! I'd like to share some things I learned doing registrtion and segmentation of large 3D biological images in the past 4 years</p>"},{"location":"tiling_artifacts_tutorial/","title":"How to get rid of tiling artifacts","text":"<p>Follow-up tutorial for </p>"},{"location":"tiling_artifacts_tutorial/#tiling-artifacts-and-trade-offs-of-feature-normalization-in-the-segmentation-of-large-biological-images-at-iccv-2025","title":"Tiling artifacts and trade-offs of feature normalization in the segmentation of large biological images at ICCV 2025","text":""},{"location":"tiling_artifacts_tutorial/#i-can-see-tile-borders-in-my-stitched-predictions-_","title":"I can see tile borders in my stitched predictions (\"\u00ac_\u00ac)","text":"<p>Biological images, especially in the case of volumetric imaging, are large. Electron microscopy volumes for connectomics or light sheet microscopy volumes can reach the scale of multiple terabytes. It is routine to work with 10-100 GB images of \\(\u223c1000^3\\) pixels, which exceed GPU memory.</p> <p>During training random patches are sampled from the annotated images and combined into a batch. During inference/evaluation, the image is split into a grid of tiles which are processed by the network independently and then stitched back to form the full prediction. Here I refer both to 2D and 3D image patches as tiles. In this setup the size of the tiles is limited from above by GPU memory and from below by the receptive field of the network. Typical tile size is around 96x96x96 - 256x256x256 pixels.</p>  <p>Tiling and stitching seem like purely technical steps, however, they can cause artifacts if predictions in neighboring tiles don't match exactly on the border. Sliding window inference is based on the idea that the predictions remain the same no matter how the tiling was made. If there is an overlap between two tiles, then in the overlapping area predictions for the same pixels should match exactly. In this case stitcing would be seamless and produce the final prediction as if we had a GPU with infinite memory.</p> <p>Unfortunately, it's not how it goes in many pipelines. The strength of tiling artifacts varies a lot depending on the specific network and dataset. It can go from barely noticeable to severely detrimental. Unfortunately, even if the artifacts are barely visible, in my experience post-processing algorithms are often very good at noticing these slight but very regular variations. For example, watershed and graph cut-based algorithms tend to make fun square-shaped instances even if the tile border is barely visible. Registration algorithms love to register stitching lines instead of focusing on the real content of the segmentation. Lastly, artifacts often become more pronounced when the network is reused for the data it was not originally trained on. All of this makes putting some effort into achieving seamless stitching well worth it. </p>"},{"location":"tiling_artifacts_tutorial/#causes-of-tiling-artifacts","title":"Causes of tiling artifacts","text":"<p>There are 4 main causes of tiling artifacts:</p> <p>1) Tile-wise normalization in pre-processing 2) Edge effects in the network predictions 3) Tile-wise feature normalization inside the network 4) Tile-wise normalization in post-processing</p> <p>Mostly when I bring up tiling artifacts, people who have experience with sliding window inference say that it's all edge effects and I just need to make overlap between tiles larger, so in this tutorial I will cover edge effects as the most widely known and easily solvable reason. </p> <p>However, it turns out that feature normalization inside the network causes much worse artifacts which can not be compensated for with simple postprocessing. These artifacts affect the whole tile, not just the edges, and strongly depend on the underlying data, making it hard to trace the source of the issue and debug it. In this tutorial I will mostly focus on the feature normalization.</p> <p>Finally, I will briefly comment on (1) and (4) - these parts are very diverse and specific to each pipeline, although there are some general rules to follow to achieve seamless final prediction.</p>"},{"location":"tiling_artifacts_tutorial/#instancenorm-refresher","title":"<code>InstanceNorm</code> refresher","text":"<p>Neural networks consist of a series of filters with normalization and nonlinearity layers in between. Feature normalization helps to make convergence faster and more stable with respect to training parameters. In most cases it's probably possible to train a network with no normalization layers at all - however, the search for optimal learning rate schedule and other parameters might require much more effort.</p> <p>The general formula for the normalization operation with input \\(x\\), output \\(y\\) is:</p> <p>$$ y = \\frac{x-\\mu}{\\sqrt{\\sigma^2 + \\epsilon}}, $$ where \\(\\mu\\) and \\(\\sigma\\) are normalization parameters and \\(\\epsilon\\) is a small constant used for numerical stability. Parameters \\(\\mu\\) and \\(\\sigma\\) can be estimated directly from the input \\(\\mu = \\mathrm{E}[x]\\) and \\(\\sigma^2 = \\mathrm{Var}[x]\\), where average can be taken either over each sample independently or over the whole batch. Alternatively, global normalization parameters independent of the current input can be used. A common strategy is to estimate the parameters as a running average over multiple batches: \\(p_{new} = (1 - momentum) \\times p_{old} + momentum \\times p_{t}\\). The update speed is determined by the \\(momentum\\), set to \\(0.1\\) by default. </p> <p>Both during training and during inference \\(InstanceNorm\\) uses statistics of each tile. This means that essentially each tile's predictions are calculated with different feature normalization parameters, making seamless stitching impossible. While it's true maths-wise, maybe it's not such a big problem because the statistics of all tiles are roughly the same? Let's look at some data.</p>"},{"location":"tiling_artifacts_tutorial/#variability-inside-the-dataset","title":"Variability inside the dataset","text":"<p>Here I load and visualize an example dataset.</p> <pre><code>from bio_monai.data import load_data_config\nfrom no_tiling.data import config_to_dataset\nimport yaml\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib import colors\nimport matplotlib.patches as mpatches\n</code></pre> <pre><code>config_path = \"experiments/em_organoids/setup_01.yaml\"\ndata_config_path = \"experiments/em_organoids/data_config_organoids_full.json\"\n\nwith open(config_path, \"r\") as yamlfile:\n    config = yaml.load(yamlfile, Loader=yaml.FullLoader)\n\ndata_config = load_data_config(data_config_path)\n\ncols = {\"fg\": \"#66c2a5\", \"bg\": \"#8da0cb\", \"boundaries\": \"#fc8d62\", \"extra\": \"#e78ac3\"}\nchannels = {\"fg\": 0, \"bg\": 3, \"boundaries\": 1, \"extra\": 2}\nfull_names = {\"fg\": \"foreground\", \"bg\": \"background\", \"boundaries\": \"boundaries\", \"extra\": \"extracellular\"}\n</code></pre> <pre><code>Read data config\nRead successful\n</code></pre> <pre><code>ds = config_to_dataset(config[\"train\"][\"dataset\"], data_config[\"full_data\"])\n</code></pre> <pre><code>Loading dataset:   0%|          | 0/1 [00:00&lt;?, ?it/s]Loading dataset: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [03:56&lt;00:00, 236.11s/it]\n</code></pre> <pre><code>data = ds[0][\"image\"].numpy()[0]\nlabel = ds[0][\"label\"].numpy().astype(np.uint16)\n</code></pre> <pre><code>xy_slice_roi = np.s_[500, :, :]\nxz_slice_roi = np.s_[:, 750, :]\nplt.figure(figsize=(20, 20))\n\nf, ax = plt.subplots(2, 2, sharey=True, figsize=(20, 20))\n\nax[0, 0].imshow(data[xy_slice_roi], cmap=\"Greys_r\")\nax[0, 0].set_xlabel(\"X\")\nax[0, 0].set_ylabel(\"Y\")\n\nax[1, 0].imshow(data[xz_slice_roi], cmap=\"Greys_r\")\nax[1, 0].set_xlabel(\"X\")\nax[1, 0].set_ylabel(\"Z\")\n\nlegend_patches = []\nfor name, chan in channels.items():\n    cmap = colors.ListedColormap(['white', cols[name]])\n    ax[0, 1].imshow(label[chan][xy_slice_roi], label=name, cmap=cmap, alpha=(label[chan][xy_slice_roi] &gt; 0).astype(float))\n    legend_patches.append(mpatches.Patch(color=cols[name], label=full_names[name]))\n\nax[1, 1].legend(handles=legend_patches, fontsize=\"large\", ncol=4)\nax[0, 1].set_xlabel(\"X\")\nax[0, 1].set_ylabel(\"Y\")\n\nlegend_patches = []\nfor name, chan in channels.items():\n    cmap = colors.ListedColormap(['white', cols[name]])\n    ax[1, 1].imshow(label[chan][xz_slice_roi], label=name, cmap=cmap, alpha=(label[chan][xz_slice_roi] &gt; 0).astype(float))\n    legend_patches.append(mpatches.Patch(color=cols[name], label=full_names[name]))\n\n\nax[0, 1].legend(handles=legend_patches, fontsize=\"large\", ncol=4)\nax[1, 1].set_xlabel(\"X\")\nax[1, 1].set_ylabel(\"Z\")\n\n\nplt.tight_layout()\n</code></pre> <pre><code>&lt;Figure size 2000x2000 with 0 Axes&gt;\n</code></pre>  <p>This is a FIB-SEM volume of a tissue and the task was to segment individual cells out. For that we need a network to perform semantic segmentation of 4 classes:</p> <ul> <li>Foreground: areas inside the cells</li> <li>Background: black area where there is no electron microscopy signal</li> <li>Boundaries: interfaces between cells</li> <li>Extracellular matrix: free space between cells</li> </ul> <p>Why this dataset? It's not really a benchmark dataset and it's legit to ask why do we care about segmenting cell boundaries in some weird organoids. In my opinion it neatly demonstrates the difficulties bioimage analysts face in every project:</p> <ul> <li>The volume is large (\\(1349 \\times 1505 \\times 1646\\) pixels) but not crazy large (fully fits into RAM on a decent machine but definitely not into GPU memory)</li> <li>The imaging method (FIB-SEM) is well-established but the sample preparation for this particular sample was quite special, so the visual appearance is unique</li> <li>Even if we imagine taking a model for cell boundaries trained on some other electron microscopy data (I'm not aware of the existance of any dataset that is even somewhat similar), it doesn't solve the problem of segmenting extracellular matrix</li> <li>Seems like <code>image == 0</code> should solve segmenting background class, but there are the lipid droplets which also have 0 signal. In principle we could apply an heuristic for segmenting background, like taking the largest connected component, but it is not transferable to other datasets (not shown here). One of the downstream tasks is counting lipid droplets per cell, so ignoring them is not an option as well  </li> <li>Cell boundaries can be super thin - down to 1 pixel wide - or very thick in different parts of the dataset, so it's not possible to downsample it further</li> <li>The dataset is quite unbalanced but the classes are distributed throughout the volume so it's not possible to pick just one foreground area manually or with any reasonable heuristic</li> </ul> <pre><code>plt.figure()\nfor name, chan in channels.items():\n    plt.bar(chan, label[chan].sum() / label[chan].size, tick_label=full_names[name], color=cols[name])\n    print(f\"{name}: {label[chan].sum() / label[chan].size:.2f}\")\nnames = channels.keys()\nplt.xticks([channels[name] for name in names], [full_names[name] for name in names]);\nplt.ylabel(\"Proportion of the class is full dataset\");\n</code></pre> <pre><code>fg: 0.56\nbg: 0.30\nboundaries: 0.09\nextra: 0.05\n</code></pre>  <p>As we can see, boundaries class takes up \\(9\\) % of the volume. Let's use <code>GridPatchDataset</code> to split the volume into a grid of non-overlapping tiles of size \\(128 \\times 128 \\times 128\\) and check the distribution of the boundaries class. \\(128 \\times 128 \\times 128\\) is a typical tile size for doing sliding window predictions with a relatively small network.</p> <pre><code>from monai.data import GridPatchDataset, DataLoader, PatchIterd\npatch_iter = PatchIterd(keys=[\"image\", \"label\"], patch_size=(128, 128, 128), start_pos=(0, 0, 0))\ngrid_dataset = GridPatchDataset(data=ds, patch_iter=patch_iter)\nfrom tqdm import tqdm\nclass_distribution = {\"fg\": [], \"boundaries\": [], \"extra\": [], \"bg\": []}\n\nfor item in tqdm(DataLoader(grid_dataset, batch_size=1, num_workers=8)):\n    item = item[0]\n    img = item[\"image\"].numpy()[0]\n    lbl = item[\"label\"].numpy()[0].astype(np.uint16)\n    class_distribution[\"fg\"].append(lbl[0].sum() / lbl[0].size)\n    class_distribution[\"boundaries\"].append(lbl[1].sum() / lbl[1].size)\n    class_distribution[\"extra\"].append(lbl[2].sum() / lbl[2].size)\n    class_distribution[\"bg\"].append(lbl[3].sum() / lbl[3].size)\n</code></pre> <pre><code>1716it [01:09, 24.55it/s]\n</code></pre> <pre><code>plt.hist(class_distribution[\"boundaries\"], bins=100, color=cols[\"boundaries\"])\nplt.ylabel(\"Number of samples\")\nplt.xlabel(\"Proportion of boundary class in tiles\")\n</code></pre> <pre><code>Text(0.5, 0, 'Proportion of boundary class in tiles')\n</code></pre>  <p>Most of the samples have no boundary class and the rest of the samples are mostly far from the dataset-wide \\(9\\)%. This means that tiles have rather different content, potentially making mean and variance used in normalization layers also very different. How large do tiles need to be roughly stable at least in terms of the class proportions?</p> <pre><code>patch_sizes = [96, 128, 192, 256, 512]\nboundary_proportions = []\n\nfor patch_size in patch_sizes:\n\n    patch_iter = PatchIterd(keys=[\"image\", \"label\"], patch_size=(patch_size, patch_size, patch_size), start_pos=(0, 0, 0))\n    grid_dataset = GridPatchDataset(data=ds, patch_iter=patch_iter)\n    from tqdm import tqdm\n    class_distribution = {\"fg\": [], \"boundaries\": [], \"extra\": [], \"bg\": []}\n\n    for item in tqdm(DataLoader(grid_dataset, batch_size=1, num_workers=8)):\n        item = item[0]\n        img = item[\"image\"].numpy()[0]\n        lbl = item[\"label\"].numpy()[0].astype(np.uint16)\n        class_distribution[\"fg\"].append(lbl[0].sum() / lbl[0].size)\n        class_distribution[\"boundaries\"].append(lbl[1].sum() / lbl[1].size)\n        class_distribution[\"extra\"].append(lbl[2].sum() / lbl[2].size)\n        class_distribution[\"bg\"].append(lbl[3].sum() / lbl[3].size)\n\n    boundary_proportions.append(class_distribution[\"boundaries\"]);\n</code></pre> <pre><code>4320it [01:12, 59.94it/s] \n1716it [01:08, 25.22it/s]\n576it [01:25,  6.71it/s]\n252it [01:40,  2.52it/s]\n36it [03:08,  5.24s/it]\n</code></pre> <pre><code>import seaborn as sns\nimport pandas as pd\n\nlong_df = pd.DataFrame(columns=[\"tile_size\", \"prop\"])\nfor patch_size, prop in zip(patch_sizes, boundary_proportions):\n    df =pd.DataFrame(data=prop, columns=[\"prop\"])\n    df[\"tile_size\"] = patch_size\n    long_df = pd.concat([long_df, df], ignore_index=True);\nlong_df[\"tile_size\"] = long_df[\"tile_size\"].astype(int)\n</code></pre> <pre><code>/tmp/ipykernel_2986358/1492769951.py:8: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n  long_df = pd.concat([long_df, df], ignore_index=True);\n</code></pre> <pre><code>plt.figure(figsize=(12, 4))\nsns.stripplot(long_df, x=\"tile_size\", y=\"prop\", hue=\"tile_size\", jitter=0.4, alpha=0.3, palette=\"husl\")\nplt.hlines(0.09, xmin=-0.5, xmax=len(patch_sizes)-0.5, color=\"green\", linestyle=\"--\", linewidth=2, label=\"Full dataset\")\nplt.xlabel(\"tile size\")\nplt.ylabel(\"Proportion of boundaries class\");\nplt.legend();\n</code></pre>  <p>As we can see, even with a very large tile size of \\(512 \\times 512 \\times 512\\), which, by the way, is unrealistic for the GPU RAM even with <code>batch_size=1</code>, the boundary class proportion significantly strays from the volume-wise \\(9\\)%. This means that the tile-wise feature statistics will vary a lot from tile to tile. In the sliding window inference the tiling is often done with overlap to compensate for the incomplete convolutions on the tile borders. However, due to tile-wise normalization inside the network predictions for the exact same pixel in neighboring tiles can be completely different even in the central part of the tile where all convolutions are valid.</p>"},{"location":"tiling_artifacts_tutorial/#feature-normalization-depends-on-tile-content","title":"Feature normalization depends on tile content","text":"<p>I trained a 3D U-Net to segment the electron microscopy volume introduced earlier. This model uses \\(InstanceNorm\\):</p> <pre><code>from monai.utils import set_determinism\nfrom monai.data import DataLoader, PatchDataset\nfrom monai.transforms import RandSpatialCropSamplesd\n\nimport torch\nfrom no_tiling.models import load_model\n\nfrom torchvision.models.feature_extraction import get_graph_node_names, create_feature_extractor\n</code></pre> <pre><code>model_path = \"data/experiments/organoids_boundaries/setup_03/model_epoch_0049.pth\"\n\n# Monai settings\nset_determinism(seed=0)\n\n# Set device\nif torch.cuda.is_available():\n    print(\"GPU is available\")\n    device = torch.device(6)\nelse:\n    print(\"GPU is not available\")\n    device = torch.device(\"cpu\")\n\n# Load model    \nmodel = load_model(model_path).to(device)\nmodel.eval()\nprint(model)\n</code></pre> <pre><code>GPU is available\nBasicUNet features: (32, 64, 128, 32).\nScaling factors: [[2, 2, 2], [2, 2, 2]]\nUNet(\n  (conv_0): TwoConv(\n    (conv_0): Convolution(\n      (conv): Conv3d(1, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n      (adn): ADN(\n        (N): InstanceNorm3d(32, eps=1e-05, momentum=0.01, affine=False, track_running_stats=False)\n        (D): Dropout3d(p=0.0, inplace=False)\n        (A): LeakyReLU(negative_slope=0.1, inplace=True)\n      )\n    )\n    (conv_1): Convolution(\n      (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n      (adn): ADN(\n        (N): InstanceNorm3d(32, eps=1e-05, momentum=0.01, affine=False, track_running_stats=False)\n        (D): Dropout3d(p=0.0, inplace=False)\n        (A): LeakyReLU(negative_slope=0.1, inplace=True)\n      )\n    )\n  )\n  (downs): ModuleList(\n    (0): Down(\n      (max_pooling): MaxPool3d(kernel_size=[2, 2, 2], stride=[2, 2, 2], padding=0, dilation=1, ceil_mode=False)\n      (convs): TwoConv(\n        (conv_0): Convolution(\n          (conv): Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n          (adn): ADN(\n            (N): InstanceNorm3d(64, eps=1e-05, momentum=0.01, affine=False, track_running_stats=False)\n            (D): Dropout3d(p=0.0, inplace=False)\n            (A): LeakyReLU(negative_slope=0.1, inplace=True)\n          )\n        )\n        (conv_1): Convolution(\n          (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n          (adn): ADN(\n            (N): InstanceNorm3d(64, eps=1e-05, momentum=0.01, affine=False, track_running_stats=False)\n            (D): Dropout3d(p=0.0, inplace=False)\n            (A): LeakyReLU(negative_slope=0.1, inplace=True)\n          )\n        )\n      )\n    )\n    (1): Down(\n      (max_pooling): MaxPool3d(kernel_size=[2, 2, 2], stride=[2, 2, 2], padding=0, dilation=1, ceil_mode=False)\n      (convs): TwoConv(\n        (conv_0): Convolution(\n          (conv): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n          (adn): ADN(\n            (N): InstanceNorm3d(128, eps=1e-05, momentum=0.01, affine=False, track_running_stats=False)\n            (D): Dropout3d(p=0.0, inplace=False)\n            (A): LeakyReLU(negative_slope=0.1, inplace=True)\n          )\n        )\n        (conv_1): Convolution(\n          (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n          (adn): ADN(\n            (N): InstanceNorm3d(128, eps=1e-05, momentum=0.01, affine=False, track_running_stats=False)\n            (D): Dropout3d(p=0.0, inplace=False)\n            (A): LeakyReLU(negative_slope=0.1, inplace=True)\n          )\n        )\n      )\n    )\n  )\n  (upcats): ModuleList(\n    (0): UpCat(\n      (upsample): UpSample(\n        (deconv): ConvTranspose3d(128, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n      )\n      (convs): TwoConv(\n        (conv_0): Convolution(\n          (conv): Conv3d(128, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n          (adn): ADN(\n            (N): InstanceNorm3d(64, eps=1e-05, momentum=0.01, affine=False, track_running_stats=False)\n            (D): Dropout3d(p=0.0, inplace=False)\n            (A): LeakyReLU(negative_slope=0.1, inplace=True)\n          )\n        )\n        (conv_1): Convolution(\n          (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n          (adn): ADN(\n            (N): InstanceNorm3d(64, eps=1e-05, momentum=0.01, affine=False, track_running_stats=False)\n            (D): Dropout3d(p=0.0, inplace=False)\n            (A): LeakyReLU(negative_slope=0.1, inplace=True)\n          )\n        )\n      )\n    )\n    (1): UpCat(\n      (upsample): UpSample(\n        (deconv): ConvTranspose3d(64, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n      )\n      (convs): TwoConv(\n        (conv_0): Convolution(\n          (conv): Conv3d(96, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n          (adn): ADN(\n            (N): InstanceNorm3d(32, eps=1e-05, momentum=0.01, affine=False, track_running_stats=False)\n            (D): Dropout3d(p=0.0, inplace=False)\n            (A): LeakyReLU(negative_slope=0.1, inplace=True)\n          )\n        )\n        (conv_1): Convolution(\n          (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n          (adn): ADN(\n            (N): InstanceNorm3d(32, eps=1e-05, momentum=0.01, affine=False, track_running_stats=False)\n            (D): Dropout3d(p=0.0, inplace=False)\n            (A): LeakyReLU(negative_slope=0.1, inplace=True)\n          )\n        )\n      )\n    )\n  )\n  (final_conv): Conv3d(32, 4, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n  (final_activation): Softmax(dim=1)\n)\n</code></pre> <p>We can directly check how tile content affects the features and predictions by sampling 4 overlapping tiles and examining the features and predictions at the exact same pixel.</p> <pre><code>image = ds[0][\"image\"]\nlabel = ds[0][\"label\"]\n</code></pre> <pre><code>gc = [200, 430, 180]\nz_sh = 32\nfull_tile_sh = 192\nmargin = 40\ntile_sh = full_tile_sh - margin\n\ntile_1 = image[:, gc[0] - z_sh:gc[0] + z_sh, gc[1] - tile_sh: gc[1] + margin, gc[2] - tile_sh: gc[2] + margin]\ntile_1_label = label[:, gc[0] - z_sh:gc[0] + z_sh, gc[1] - tile_sh: gc[1] + margin, gc[2] - tile_sh: gc[2] + margin]\ntile_1_coord = [z_sh, tile_sh, tile_sh]\n\ntile_2 = image[:, gc[0] - z_sh:gc[0] + z_sh, gc[1] - tile_sh: gc[1] + margin, gc[2] - margin: gc[2] + tile_sh]\ntile_2_label = label[:, gc[0] - z_sh:gc[0] + z_sh, gc[1] - tile_sh: gc[1] + margin, gc[2] - margin: gc[2] + tile_sh]\ntile_2_coord = [z_sh, tile_sh, margin]\n\ntile_3 = image[:, gc[0] - z_sh:gc[0] + z_sh, gc[1] - margin: gc[1] + tile_sh, gc[2] - tile_sh: gc[2] + margin]\ntile_3_label = label[:, gc[0] - z_sh:gc[0] + z_sh, gc[1] - margin: gc[1] + tile_sh, gc[2] - tile_sh: gc[2] + margin]\ntile_3_coord = [z_sh, margin, tile_sh]\n\ntile_4 = image[:, gc[0] - z_sh:gc[0] + z_sh, gc[1] - margin: gc[1] + tile_sh, gc[2] - margin: gc[2] + tile_sh]\ntile_4_label = label[:, gc[0] - z_sh:gc[0] + z_sh, gc[1] - margin: gc[1] + tile_sh, gc[2] - margin: gc[2] + tile_sh]\ntile_4_coord = [z_sh, margin, margin]\n</code></pre> <pre><code>plt.figure(figsize=(10, 10))\nplt.imshow(image[0, gc[0], :, :], cmap=\"Greys_r\")\nplt.scatter(gc[2], gc[1], color=\"red\")\nplt.vlines([gc[2] - tile_sh, gc[2] + tile_sh], ymin=gc[1] - tile_sh, ymax=gc[1] + tile_sh, linewidth=3, color=\"aquamarine\")\nplt.hlines([gc[1] - tile_sh, gc[1] + tile_sh], xmin=gc[2] - tile_sh, xmax=gc[2] + tile_sh, linewidth=3, color=\"aquamarine\")\nplt.xlabel(\"X\")\nplt.ylabel(\"Y\");\n</code></pre>  <p>Here red dot shows the pixel that we are going to examine. I'm going to split the area highlighted with a square into 4 overlapping tiles. In this case the neighboring tiles have quite different class distributions: upper left tile has more extracellular matrix and outside-of-image background while tile lower right tile is mostly inside one cell, with most of the pixels having foreground class:</p> <pre><code>f, axes = plt.subplots(2, 2,  sharex=True, sharey=True, figsize=(12, 12))\n\nfor tile, label, coord, ax, idx in zip([tile_1, tile_2, tile_3, tile_4], [tile_1_label, tile_2_label, tile_3_label, tile_4_label], [tile_1_coord, tile_2_coord, tile_3_coord, tile_4_coord], axes.flat, range(4)):\n    ax.imshow(tile[0, z_sh], cmap=\"Greys_r\")\n    ax.scatter([coord[2]], [coord[1]], color=\"red\")\n    ax.set_title(f\"Tile {idx} raw data\")\n\nplt.show();\nplt.tight_layout();\n</code></pre>  <pre><code>&lt;Figure size 640x480 with 0 Axes&gt;\n</code></pre> <pre><code>f, axes = plt.subplots(2, 2,  sharex=True, sharey=True, figsize=(12, 12))\n\nfor tile, label, coord, ax, idx in zip([tile_1, tile_2, tile_3, tile_4], [tile_1_label, tile_2_label, tile_3_label, tile_4_label], [tile_1_coord, tile_2_coord, tile_3_coord, tile_4_coord], axes.flat, range(4)):\n    # ax.imshow(tile[0, z_sh], cmap=\"Greys_r\")\n    legend_patches = []\n    for name, chan in channels.items():\n        cmap = colors.ListedColormap(['white', cols[name]])\n        ax.imshow(label[chan, z_sh], label=name, cmap=cmap, alpha=(label[chan, z_sh] &gt; 0).astype(float))\n        legend_patches.append(mpatches.Patch(color=cols[name], label=full_names[name]))\n    ax.scatter([coord[2]], [coord[1]], color=\"red\")\n    ax.set_title(f\"Tile {idx} ground truth\")\n\naxes[0, 1].legend(handles=legend_patches, fontsize=\"large\", ncol=4)\nplt.show();\nplt.tight_layout();\n</code></pre>  <pre><code>&lt;Figure size 640x480 with 0 Axes&gt;\n</code></pre> <p>The network starts with the convolution <code>conv_0</code> followed by a normalization layer and nonlinearity. I extracted the outputs of the very first convolution and the subsequent normalization layer for the pixel of interest. Since it is exactly the same pixel with the same immediate context, in principle it should also have the same or very similar features inside the network regardless of how the tiling was done, right?</p> <pre><code>from torchvision.models.feature_extraction import get_graph_node_names, create_feature_extractor\nnodes, _ = get_graph_node_names(model)\nprint(nodes)\n</code></pre> <pre><code>['x', 'conv_0.conv_0.conv', 'conv_0.conv_0.adn.N', 'conv_0.conv_0.adn.D', 'conv_0.conv_0.adn.A', 'conv_0.conv_1.conv', 'conv_0.conv_1.adn.N', 'conv_0.conv_1.adn.D', 'conv_0.conv_1.adn.A', 'downs.0.max_pooling', 'downs.0.convs.conv_0.conv', 'downs.0.convs.conv_0.adn.N', 'downs.0.convs.conv_0.adn.D', 'downs.0.convs.conv_0.adn.A', 'downs.0.convs.conv_1.conv', 'downs.0.convs.conv_1.adn.N', 'downs.0.convs.conv_1.adn.D', 'downs.0.convs.conv_1.adn.A', 'downs.1.max_pooling', 'downs.1.convs.conv_0.conv', 'downs.1.convs.conv_0.adn.N', 'downs.1.convs.conv_0.adn.D', 'downs.1.convs.conv_0.adn.A', 'downs.1.convs.conv_1.conv', 'downs.1.convs.conv_1.adn.N', 'downs.1.convs.conv_1.adn.D', 'downs.1.convs.conv_1.adn.A', 'upcats.0.upsample.deconv', 'upcats.0.convs.conv_0.conv', 'upcats.0.convs.conv_0.adn.N', 'upcats.0.convs.conv_0.adn.D', 'upcats.0.convs.conv_0.adn.A', 'upcats.0.convs.conv_1.conv', 'upcats.0.convs.conv_1.adn.N', 'upcats.0.convs.conv_1.adn.D', 'upcats.0.convs.conv_1.adn.A', 'upcats.1.upsample.deconv', 'upcats.1.convs.conv_0.conv', 'upcats.1.convs.conv_0.adn.N', 'upcats.1.convs.conv_0.adn.D', 'upcats.1.convs.conv_0.adn.A', 'upcats.1.convs.conv_1.conv', 'upcats.1.convs.conv_1.adn.N', 'upcats.1.convs.conv_1.adn.D', 'upcats.1.convs.conv_1.adn.A', 'final_conv', 'final_activation']\n</code></pre> <pre><code>feature_extractor = create_feature_extractor(\nmodel, return_nodes=['conv_0.conv_0.conv', 'conv_0.conv_0.adn.N'])\n\nconv_features = []\nnorm_features = []\nprediction = []\n\nf, axes = plt.subplots(2, 2,  sharex=True, sharey=True, figsize=(12, 12))\n\nfor tile, coord, ax, idx in zip([tile_1, tile_2, tile_3, tile_4], [tile_1_coord, tile_2_coord, tile_3_coord, tile_4_coord], axes.flat, range(4)):\n    out = feature_extractor(tile[None, ...].to(device))\n    conv_feat = out['conv_0.conv_0.conv'].detach().to(\"cpu\")\n    norm_feat = out['conv_0.conv_0.adn.N'].detach().to(\"cpu\")\n\n    ax.bar(range(conv_feat.shape[1]), conv_feat[0, :, coord[0], coord[1], coord[2]])\n    ax.set_title(f\"Convolution 0, tile {idx}\")\n\n\n    conv_features.append(conv_feat[0, :, coord[0], coord[1], coord[2]])\n    norm_features.append(norm_feat[0, :, coord[0], coord[1], coord[2]])\n</code></pre>  <p>As expected, the output of the first convolution layer is exactly the same for the target pixel, regardless of the tile.</p> <pre><code>f, axes = plt.subplots(2, 2,  sharex=True, sharey=True, figsize=(12, 12))\n\nfor tile, coord, ax, idx in zip([tile_1, tile_2, tile_3, tile_4], [tile_1_coord, tile_2_coord, tile_3_coord, tile_4_coord], axes.flat, range(4)):\n    norm_feat = norm_features[idx]\n    conv_feat = conv_features[idx]\n\n    ax.bar(range(len(norm_feat)), norm_feat, color=\"crimson\", label=\"Convolution + InstanceNorm\")\n    ax.bar(range(len(conv_feat)), conv_feat, alpha=0.5, label=\"Convolution\")\n    ax.set_title(f\"Convolution + InstanceNorm, tile {idx}\")\n\naxes[0, 0].legend()\n</code></pre> <pre><code>&lt;matplotlib.legend.Legend at 0x7ffedeaf62d0&gt;\n</code></pre>  <p>On this plot feature after the first convolution layer are shown in transparent blue and features after normalization - in red. After normalization the features for the same pixel become different, depending on which tile it belongs to. Distributions in tile 1 and tile 3 are more or less similar because the content of these tiles is more similar, however, even between them there's a significant difference. </p> <pre><code>f, axes = plt.subplots(2, 2,  sharex=True, sharey=True, figsize=(12, 12))\n\nfor tile, coord, ax, idx in zip([tile_1, tile_2, tile_3, tile_4], [tile_1_coord, tile_2_coord, tile_3_coord, tile_4_coord], axes.flat, range(4)):\n    out = model(tile[None, ...].to(device)).detach().to(\"cpu\").numpy()\n    ax.imshow(out[0, 0, z_sh], cmap=\"Greys_r\")\n    ax.scatter([coord[2]], [coord[1]], color=\"red\")\n    print(f\"Class probabilities at target pixel in tile {idx}\")\n    print(out[0, :, z_sh, coord[1], coord[2]])\n    ax.set_title(f\"Prediction of boundaries class, tile {idx}\")\n</code></pre> <pre><code>Class probabilities at target pixel in tile 0\n[9.9999976e-01 6.6844350e-08 3.7892136e-08 6.6073149e-08]\nClass probabilities at target pixel in tile 1\n[9.9873453e-01 6.2038205e-05 1.8355773e-04 1.0198853e-03]\nClass probabilities at target pixel in tile 2\n[9.9901414e-01 3.3465363e-05 9.1410409e-05 8.6091616e-04]\nClass probabilities at target pixel in tile 3\n[9.9947053e-01 9.7691009e-05 1.9195935e-04 2.3982450e-04]\n</code></pre>  <p>Difference in normalized features leads to the differences in final predictions. In this particular case the difference was not big enough to flip the most probable class for this pixel, but still the predictions do not match exactly anymore. 0.01 might be imperceptible for a person but I promise that watershed will pick up on it in the most annoying way! Besides, if we compare other areas, we can see that some pixels which are confidently predicted as boundary in one tile can change completely to a different class in another tile.</p>"},{"location":"tiling_artifacts_tutorial/#prediction-quality-depends-on-tile-size","title":"Prediction quality depends on tile size","text":"<p>The size of tiles defines how stable the normalization statistics are and therefore how well the predictions match between tiles. Running prediction with different tile sizes highlights the problem and gives another confirmation that the visible tiles are not an artifact of data acquisition.  I run predictions with the following tile sizes:  \\(96 \\times 96 \\times 96\\) \\(96 \\times 128 \\times 128\\) \\(96 \\times 256 \\times 256\\) </p> <p>Note that there's an overlap of \\(32\\) pixels between tiles in each direction and then the predictions in the \\(32\\) pixels halo are multiplied by \\(0\\). This way only valid areas of each tile are used in the stitched prediction. For example, for \\(96 \\times 96 \\times 96\\) tile only the central area of \\(32 \\times 32 \\times 32\\) is used in the final stitched volume. In this setup there's no averaging between predictions from neighbouring tiles.</p> <pre><code>from monai.inferers import sliding_window_inference\n</code></pre> <pre><code>image = ds[0][\"image\"]\nz_pos = image.shape[1] // 2\ninference_slice = image[None, :, z_pos - 48:z_pos + 48, ...]\n</code></pre> <pre><code>tile_sizes = [96, 128, 256]\ntile_size_predictions = []\n\nfor tile_size in tile_sizes:\n    roi_size=(96, tile_size, tile_size)\n    print(f\"Predict using tile shape {roi_size}\")\n    halo=(32, 32, 32)\n    sw_batch_size=1\n    roi_weight_map = np.zeros(roi_size)\n    roi_weight_map[halo[0]:-halo[0], halo[1]:-halo[1], halo[2]:-halo[2]] = 1\n    overlap = max(*halo) * 2 / max(*roi_size)\n    print(\"Overlap: \", overlap)\n    with torch.no_grad():\n        prediction = sliding_window_inference(inference_slice, roi_size, sw_batch_size, model, progress=True, sw_device=torch.device(6), device=\"cpu\", roi_weight_map=roi_weight_map, overlap=overlap)\n        prediction = prediction.detach().to(\"cpu\")\n    tile_size_predictions.append(prediction)\n</code></pre> <pre><code>Predict using tile shape (96, 96, 96)\nOverlap:  0.6666666666666666\n\n\n  1%|          | 13/2300 [00:01&lt;02:13, 17.16it/s]100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2300/2300 [01:45&lt;00:00, 21.76it/s]\n\n\nPredict using tile shape (96, 128, 128)\nOverlap:  0.5\n\n\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 575/575 [00:45&lt;00:00, 12.70it/s]\n\n\nPredict using tile shape (96, 256, 256)\nOverlap:  0.25\n\n\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 72/72 [00:20&lt;00:00,  3.59it/s]\n</code></pre> <pre><code>prediction = prediction.detach().to(\"cpu\")\n</code></pre> <pre><code>f, axes = plt.subplots(1, 3,  sharex=True, sharey=True, figsize=(16, 5))\nfor tile_size, prediction, ax in zip(tile_sizes, tile_size_predictions, axes.flat):\n    ax.imshow(prediction[0, 1, 48], cmap=\"Greys_r\")\n    ax.set_xlabel(\"X\")\n    ax.set_xlabel(\"Y\")\n    ax.set_title(f\"Cell boundary prediction, tile size {tile_size}\")\nplt.show()\n\nf, axes = plt.subplots(1, 3,  sharex=True, sharey=True, figsize=(16, 5))\nfor tile_size, prediction, ax in zip(tile_sizes, tile_size_predictions, axes.flat):\n    ax.imshow(prediction[0, 0, 48], cmap=\"Greys_r\")\n    ax.set_xlabel(\"X\")\n    ax.set_xlabel(\"Y\")\n    ax.set_title(f\"Foreground prediction, tile size {tile_size}\")\nplt.show()\n\nf, axes = plt.subplots(1, 3,  sharex=True, sharey=True, figsize=(16, 5))\nfor tile_size, prediction, ax in zip(tile_sizes, tile_size_predictions, axes.flat):\n    ax.imshow(prediction[0, 2, 48], cmap=\"Greys_r\")\n    ax.set_xlabel(\"X\")\n    ax.set_xlabel(\"Y\")\n    ax.set_title(f\"Extracellular matrix prediction, tile size {tile_size}\")\nplt.show()\n</code></pre>    <p>Inspecting the predictions, we can note a few things:</p> <ul> <li> <p>The performance very obviously depends on the tile size. Also there are clear squares, size of which corresponds to the chosen tile size minus halo. This is kind of embarrassing? At the time of writing, it's end of 2025, GenAI is putting artists, creative writers and python developers out of jobs, technological singularity is quickly approaching, etc etc, and yet, what's this?? I feel kind of insecure posting these ugly predictions on the internet. Maybe I should just smooth out the tile edges somehow before presenting this and pretend it's fine... Oh wait, too late...</p> </li> <li> <p>Mismatch is not limited to the edges of the tiles. In fact, the edges (32 pixels wide) were removed before stitching. Tiling artifacts affect the entire tile.</p> </li> <li> <p>It's not a slight difference. Looking closely at the extracellular matrix predictions, especially at the larger tile size, we can notice that this class is predicted correctly in the tiles which contain both foreground and extracellular matrix, but then if the tile entirely consists only of extracellular matrix, it is suddenly wrongly predicted as foreground. The same can be seen in the middle of the cells: if a tile fits completely inside the cell and there are no cell boundaries for the context, normalization becomes wrong.</p> </li> </ul>"},{"location":"tiling_artifacts_tutorial/#global-feature-normalization-eliminates-tiling-artifacts","title":"Global feature normalization eliminates tiling artifacts","text":"<p>The problems with stitching are caused by the feature normalization inside the network, so, to get rid of the tiling artifacts, a normalization that uses the same parameters for all the tiles should be used. \\(BatchNorm\\), another popular normalization layer, is doing just that. During training \\(BatchNorm\\) uses statistics of the current batch and collects a running average. During inference it uses saved running average. Although running mean and variance are not learned in a sense that there's no gradient descent with respect to these parameters, they are estimated from the training data. With the normalization parameters fixed, the predictions can match exactly.</p> <p>Running predictions with different tile size shows that for \\(InstanceNorm\\) the prediction quality depends on the tile size. Once the tile size is large enough that most tiles become more or less representative of the full dataset, the prediction quality reaches plato. For a network with global feature normalization the prediction quality remains the same regardless of the tile size.</p>"},{"location":"tiling_artifacts_tutorial/#receptive-field-and-halo","title":"Receptive field and halo","text":"<p>Neural networks used for segmentation take images as input and produce images, usually of the same size, as output. The value assigned by the network to each pixel of the output is a deterministic function of a certain set of pixels in the input. This area of the input that affects one pixel in the output is called receptive field. There are many papers and tutorials describing how to calculate its size theoretically, based on the convolution filter sizes and strides. I have to admit that I never managed to apply these formulas correctly, somehow there's always an extra convolution layer or something... </p> <p>Fortunately, we can just use backpropagation to calculate theoretical and real receptive field. For that we run predictions with the network as usual and then calculate the gradient of the prediction in the central pixel with respect to the input image (not the network parameters like we do during training). The area of input where the gradients are not <code>None</code> is the theoretical receptive field and the area where the gradients are large is the effective receptive field. The gradient measurement from just one tile can be noisy so the gradients are calculated for all the tiles in the dataset and then averaged.</p> <p>Let's compare the receptive field for to networks with the same architecture, except one has <code>BatchNorm</code> (global feature normalization) and the other - <code>InstanceNorm</code> (tile-wise feature normalization).</p>  <p>U-Net with global feature normalization has the receptive field that I would expect upon reading about receptive fields of CNNs in general. There is a limited area around the target pixel - in this case, a square with a side of 44 pixels - which takes part in the calculation of the final prediction in the central pixel of the output. Within this area the gradients have roughly gaussian distribution, with the relative contribution of pixels to the prediction falling for pixels which are further away from the center of the image. For this network only the content of the receptive field matters, making it possible to seamlessly stitch predictions as long as the 22 pixels on the edge of each tile are removed due to incomplete context.</p> <p>U-Net with tile-wise feature normalization has the same effective receptive field, determined by the number of layers and parameters of the convolution layers. However, its full receptive field is not limited and takes up the whole tile, as through the normalization parameter calculation every pixel takes part in the final prediction. This network's predictions can't be used to obtain seamless stitched predictions regardless of how large the overlap between tiles is.</p>"},{"location":"tiling_artifacts_tutorial/#transformers-inherently-cause-tiling-artifacts","title":"Transformers inherently cause tiling artifacts","text":"<p>For the convolutional architecture full inference pipeline for the large image is the following:</p> <ul> <li>Split the large image into tiles of arbitrary size</li> <li>Run prediction with features inside the network normalized either using statistics of the input tile or global fixed parameters</li> <li>Stitch the predictions for individual tiles to obtain final prediction for the large image</li> </ul> <p>For Transformers the procedure is more complicated:</p> <ul> <li>Split the large image into tiles of fixed size defined by Transformer architecture, in particular, by the positional encoding</li> <li>Split the tile into patches, often \\(16 \\times 16\\) is used</li> <li>Encode each patch, normalize features per patch using <code>GroupNorm</code></li> <li>Calculate new features of each patch using features of all patches and attention mechanism, normalize features per patch using <code>GroupNorm</code>, repeat this step multiple times</li> <li>Use final features to generate dense prediction</li> <li>Stitch the predictions for individual tiles to obtain final prediction for the large image</li> </ul> <p>In this case the whole tile is always used for the prediction of every pixel of the output. Normally the selling point of transformer architecture: less bias, more context used, but in the sliding window inference context it means that it's not possible to achieve seamless stitching. Moreover, since there's a second round of tiling/stitching inside the transformer, dense predictions even inside one tile are not necessarily smooth - for more details see the numerous works on the registers and high-norm patches in DINOv2. </p> <p>The same receptive field calculation as we did for the U-Net, shows the clear patch artifacts in the Transformer-based UNETR. Interestingly, the effective receptive field is not larger than for the usual U-Net:</p>  <p>In transformers theoretical receptive field is the whole tile, so mismatch between tiles is not an edge artifact but rather the inherent property of the network arising from the attention mechanism and patch-wise normalization.</p>"},{"location":"tiling_artifacts_tutorial/#tile-mismatch-metric","title":"Tile mismatch metric","text":"<p>Quantification of tiling artifacts is challenging because their magnitude depends on the similarity of the content of neighboring tiles in different parts of the image. To figure out if there's tiling mismatch, split the image into overlapping tiles and compare the predictions in the overlap areas. To avoid edge effects, only take the valid part of the overlap, as shown in the picture above. To catch all possible issues, run the predictions for the whole dataset in a way that valid overlap areas cover the whole image or at least some representative part of it.</p> <p>I found it useful to calculate two metrics: maximum distance and median tile mismatch.</p> <p>Maximum distance is the maximum difference between predictions. $$ max\\ dist = \\max_{i=1}^M(|O_{i1} - O_{i2}|), $$</p> <p>where \\(O_{i1}\\) and \\(O_{i2}\\) are predictions in the valid overlap region in tile \\(i\\) and \\(M\\) is number of sampled tiles. In \\(max\\ dist\\) \\(\\max\\) is taken over all channels and all tiles, making it an indicator of whether given setup produces tiling artifacts. If \\(max\\ dist = 0\\) the predictions match perfectly and the pipeline is fully artifact-free. Although this metric is good for detecting the presence of artifacts, I found that in almost all cases the maximum difference was not very descriptive of how bad the artifacts are on average. For example, in our previous experiment with different tile sizes, prediction with \\(96 \\times 96 \\times 96\\) obviously had worse tiling artifacts than \\(96 \\times 128 \\times 128\\). Maximum distance in both cases is 1 as there are clearly pixels that completely change the class but on average we see less artifacts.</p> <p>To quantify how bad the artifacts are I found it useful to look at the median distance between predictions over all tiles:</p> \\[ tile\\ mismatch = \\mathop{\\mathit{median}}_{i=1}^{M}(1 - Dice(O_{i1}, O_{i2})),  \\] <p>\\(Tile\\ mismatch\\) is calculated per channel and characterizes the magnitude of the artifacts. It is likely that in a large dataset there are neighboring tiles with similar content which have very similar predictions in the overlapping areas and then there are some tiles where the content of the image changes suddenly, causing much stronger artifacts. Calculating median roughly describes how bad the artifacts are in the image overall.</p>"},{"location":"tiling_artifacts_tutorial/#practical-tips","title":"Practical tips","text":""},{"location":"tiling_artifacts_tutorial/#how-to-detect-tiling-artifacts","title":"How to detect tiling artifacts?","text":"<ul> <li>Run sliding window inference prediction pipeline with different tile sizes and different tile overlaps. If predictions change - there is something going on</li> <li>Run predictions for the tiles with large overlap and calculate the maximum distance between predictions. It should be exactly 0.</li> </ul>"},{"location":"tiling_artifacts_tutorial/#how-to-eliminate-tiling-artifacts","title":"How to eliminate tiling artifacts?","text":"<p>Once it's established that there is an issues with tiling and stitching, the question is which part of the pipeline causes them.</p>"},{"location":"tiling_artifacts_tutorial/#preprocessing","title":"Preprocessing","text":"<p>After the large image is split into tiles, some pipelines apply transforms to tiles separately. For example, it is relatively common to apply normalize input intensity distribution for each tile separately, changing the input of the network in tile-dependent manner. Of course, this causes tiling artifacts. You can think of it the following way: if even the network input can't be stitched back seamlessly, why would the output have artifact-free?</p> <p>No tile-wise transforms should be applied to the input data</p>"},{"location":"tiling_artifacts_tutorial/#feature-normalization-inside-the-network","title":"Feature normalization inside the network","text":"<p>As we have seen in this tutorial, convolutional networks which normalize features based on the current input rather than global fixed parameters, cause mismatch between predictions. Unfortunately, it's not possible to easily switch between <code>InstanceNorm</code> and <code>BatchNorm</code>, so the only way is to train another network. Of course, there are many different architectures and layers, and a simple replacement of a normalization layer might not be enough for more sophisticated. No matter the architecture, the following principle hold:   </p> <p>All network parameters should be global and independent of the current input</p> <p>Of course, it applies only to networks which are translation-invariant. For examples, transformers always use the whole input image to generate predictions, so it is not possible to avoid artifacts.</p>"},{"location":"tiling_artifacts_tutorial/#tile-overlap","title":"Tile overlap","text":"<p>When the network is applied to the tiles, usually zero-padding or mirroring is used to keep the output same size as input. The predictions on the edge of the tiles will not match between neighboring tiles because the network does not get correct full context: part of the input is replaced with zeros. The size of affected area is determined by the theoretical receptive field of the network. Removing this affected area ensures that the predictions don't have any edge effects and should match.</p> <p>Only valid part of the prediction should be used for stitching</p>"},{"location":"tiling_artifacts_tutorial/#postprocessing","title":"Postprocessing","text":"<p>Same as with preprocessing, there should be no tile-wise postprocessing. </p> <p>No tile-wise transforms should be applied to the output</p>"},{"location":"tiling_artifacts_tutorial/#consider-the-whole-pipeline","title":"Consider the whole pipeline","text":"<p>If the pipeline has tiling artifacts, it is likely that the problem is not just in one part of the pipeline. For example, if the network creates artifacts, postprocessing might be set up to reduce them. Maybe the predicted probabilities are additionally normalized to reduce mismatch - then after fixing the problems with the network this postprocessing is likely to make the predictions worse. If after fixing one step the artifacts don't go away or get worse - it just means that there were multiple causes for them. Try to isolate parts: - stitch the preprocessed inputs by replacing the network with an <code>Identity</code> layer - run the network on the overlapping tile which were created manually, not by your normal sampling pipeline</p> <p>Bioimage analysis is a diverse field and I couldn't possible anticipate all the fun/weird things people do to their images, however if the network is translation-invariant and none of its parameters depend on the current input, it generally should be possible to achieve seamless predictions for the images of arbitrary size. </p>"},{"location":"tiling_artifacts_tutorial/#how-to-minimize-tiling-artifacts","title":"How to minimize tiling artifacts?","text":"<p>Let's say the network which causes tiling artifacts is fixed and can't be retrained or you decided to use transformer architecture. How to make sure that there are as little artifacts as possible?</p> <ul> <li> <p>Calculate effective receptive field and set the halo to a reasonable value. For example, for the UNETR used in this tutorial, we can see that only a few central tiles have a strong influence on the prediction, therefore <code>32</code> pixels is a good halo size.</p> </li> <li> <p>Adjust tile sampling to make sure that the content of all tiles is similar. Usually it means using <code>batch_size=1</code> and as large tile size as possible. Try to crop as much background as possible. </p> </li> <li> <p>If, despite all efforts, there are still visible borders between tiles, try to apply smoothing when stitching. For example, nnU-Net and Cellpose compute weighted average of the tiles, with the weight decreasing from 1 in the middle of the tile to 0 at the border, so even if there's mismatch, it's not so obvious.</p> </li> </ul>"},{"location":"tiling_artifacts_tutorial/#why-use-instancenorm-at-all","title":"Why use <code>InstanceNorm</code> at all?","text":"<p>If <code>InstanceNorm</code> causes all these problems, why does anyone use it at all? For detailed answer, check out my tutorial on <code>BatchNorm</code>.</p> <p>For examples of different modalities, architectures and quantitative results check out my manuscript Tiling artifacts and trade-offs of feature normalization in the segmentation of large biological images.</p>"},{"location":"train-eval-disparity/","title":"Is <code>BatchNorm</code> a good choice for my problem?","text":"<p>Follow-up tutorial for </p>"},{"location":"train-eval-disparity/#tiling-artifacts-and-trade-offs-of-feature-normalization-in-the-segmentation-of-large-biological-images-at-iccv-2025","title":"Tiling artifacts and trade-offs of feature normalization in the segmentation of large biological images at ICCV 2025","text":""},{"location":"train-eval-disparity/#my-model-doesnt-do-the-thing","title":"My model doesn't do the thing \u02d9\u25e0\u02d9","text":"<p>During training the loss confidently goes down and the model makes correct or at least reasonable predictions. Sometimes it is even correct in the cases where the ground truth label is obviously wrong! Awesome. However, in the evaluation the predictions don't look so good. Even when the training and evaluation data come from the same source and have no perceptual difference.</p> <p>The common knowledge explanation is overfitting. Overfitting means that there is a substantial difference between training and evaluation data so the statistical distribution learned by the model based on trainig data doesn't describe what's going on in the evaluation data, leading to prediction errors. There are two widely known remedies for overfitting: adding more training data and augmentations.</p> <p>Adding more training data is pretty straightforward: we check what kind of errors the model made on test data, annotate a few hard cases and add these annotations to the training data. Now training data more faithfully reflects the target distribution and predictions should get better. However it's not always possible to annotate more data, so instead we can try to make training data look a bit more similar to the evaluation data by applying some random transformations. For example, if we know that the evaluation data contains some images with less contrast or with some defocused areas, we can apply contrast adjustment or gaussian blur to some images in the training dataset. Now the model \"has seen\" these examples and will not make so many mistakes during evaluation.</p> <p>Makes sense, right? Any machine learning course teaches this within the first 2-3 lectures.</p> <p>However, the longer I worked on biological image segmentation, the less it seemed that overfitting is the answer. I made the following observations:</p> <ul> <li>Often the errors made by pretrained models made no sense. It wasn't like \"my images are a little brighter than the training set on average so the model predicts too much foreground\". It was more of \"predict foreground more or less correctly and then also randomly find something in the background\" or \"make more or less correct predictions in half the image and then completely fail in the other exactly same-looking half\". </li> <li>Augmentations made barely any difference in training results. Sure, if I went way too hard on intensity augmentations, I could make the training diverge and fail completely, but the reasonable rotation, gaussian noise and slight contrast adjustments never did anything, whether I just put in the strongest augmentations I could or spent hours inspecting raw data and adjusting augmentations to match the natural variability of the data. </li> <li>(gasp) Adding more ground truth didn't always make results better.</li> </ul> <p>Turns out that there's another, normally overlooked, source of failure: train/evaluation disparity. In this tutorial I will explain what it is, show why it's so ubiquitous in bioimage analysis and suggest some ways to avoid it in your particular pipeline. </p> <p>In this tutorial we will explore the effect of batch size and dataset diversity using cell boundary segmentation in electron microscopy with 3D U-Net as an example. Here we will use a modified U-Net implementation from Monai, however, the same analysis can be done for any dataset and any CNN-based pipeline.</p>"},{"location":"train-eval-disparity/#sliding-window-inference-and-patch-sampling-in-training","title":"Sliding window inference and patch sampling in training","text":"<p>Biological images, especially in the case of volumetric imaging, are large. Electron microscopy volumes for connectomics or light sheet microscopy volumes can reach the scale of multiple terabytes. It is routine to work with 10-100 GB images of \\(\u223c1000^3\\) pixels, which exceed GPU memory.</p> <p>During training random patches are sampled from the annotated images and combined into a batch. During inference/evaluation, the image is split into a grid of tiles which are predicted independently and then stitched back to form the full prediction. Here I refer both to 2D and 3D image patches as tiles. In this setup the size of the tiles is limited from above by GPU memory and from below by the receptive field of the network. Typical tile size is around 96x96x96 - 256x256x256 pixels and the training batch size is 2-3 tiles per batch. During inference it is not necessary to store gradients, therefore either tile size or batch size can be larger.</p>"},{"location":"train-eval-disparity/#diversity-of-biological-datasets","title":"Diversity of biological datasets","text":"<p>Another important feature of biological images is extreme class disbalance. Within one image whole areas can contain no foreground objects. Let's inspect one electron microscopy volume as an example.</p> <pre><code>from bio_monai.data import load_data_config\nfrom no_tiling.data import config_to_dataset\nimport yaml\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib import colors\nimport matplotlib.patches as mpatches\n</code></pre> <pre><code>config_path = \"experiments/em_organoids/setup_01.yaml\"\ndata_config_path = \"experiments/em_organoids/data_config_organoids_full.json\"\n\nwith open(config_path, \"r\") as yamlfile:\n    config = yaml.load(yamlfile, Loader=yaml.FullLoader)\n\ndata_config = load_data_config(data_config_path)\n\ncols = {\"fg\": \"#66c2a5\", \"bg\": \"#8da0cb\", \"boundaries\": \"#fc8d62\", \"extra\": \"#e78ac3\"}\nchannels = {\"fg\": 0, \"bg\": 3, \"boundaries\": 1, \"extra\": 2}\nfull_names = {\"fg\": \"foreground\", \"bg\": \"background\", \"boundaries\": \"boundaries\", \"extra\": \"extracellular\"}\n</code></pre> <pre><code>Read data config\nRead successful\n</code></pre> <pre><code>ds = config_to_dataset(config[\"train\"][\"dataset\"], data_config[\"full_data\"])\n</code></pre> <pre><code>Loading dataset: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [03:39&lt;00:00, 219.93s/it]\n</code></pre> <pre><code>data = ds[0][\"image\"].numpy()[0]\nlabel = ds[0][\"label\"].numpy().astype(np.uint16)\n</code></pre> <pre><code>xy_slice_roi = np.s_[500, :, :]\nxz_slice_roi = np.s_[:, 750, :]\nplt.figure(figsize=(20, 20))\n\nf, ax = plt.subplots(2, 2, sharey=True, figsize=(20, 20))\n\nax[0, 0].imshow(data[xy_slice_roi], cmap=\"Greys_r\")\nax[0, 0].set_xlabel(\"X\")\nax[0, 0].set_ylabel(\"Y\")\n\nax[1, 0].imshow(data[xz_slice_roi], cmap=\"Greys_r\")\nax[1, 0].set_xlabel(\"X\")\nax[1, 0].set_ylabel(\"Z\")\n\nlegend_patches = []\nfor name, chan in channels.items():\n    cmap = colors.ListedColormap(['white', cols[name]])\n    ax[0, 1].imshow(label[chan][xy_slice_roi], label=name, cmap=cmap, alpha=(label[chan][xy_slice_roi] &gt; 0).astype(float))\n    legend_patches.append(mpatches.Patch(color=cols[name], label=full_names[name]))\n\nax[1, 1].legend(handles=legend_patches, fontsize=\"large\", ncol=4)\nax[0, 1].set_xlabel(\"X\")\nax[0, 1].set_ylabel(\"Y\")\n\nlegend_patches = []\nfor name, chan in channels.items():\n    cmap = colors.ListedColormap(['white', cols[name]])\n    ax[1, 1].imshow(label[chan][xz_slice_roi], label=name, cmap=cmap, alpha=(label[chan][xz_slice_roi] &gt; 0).astype(float))\n    legend_patches.append(mpatches.Patch(color=cols[name], label=full_names[name]))\n\n\nax[0, 1].legend(handles=legend_patches, fontsize=\"large\", ncol=4)\nax[1, 1].set_xlabel(\"X\")\nax[1, 1].set_ylabel(\"Z\")\n\n\nplt.tight_layout()\n</code></pre> <pre><code>&lt;Figure size 2000x2000 with 0 Axes&gt;\n</code></pre>  <p>This is a FIB-SEM volume of a tissue and the task was to segment individual cells out. For that we need a network to perform semantic segmentation of 4 classes:</p> <ul> <li>Foreground: areas inside the cells</li> <li>Background: black area where there is no electron microscopy signal</li> <li>Boundaries: interfaces between cells</li> <li>Extracellular matrix: free space between cells</li> </ul> <p>Why this dataset? It's not really a benchmark dataset and it's legit to ask why do we care about segmenting cell boundaries in some weird organoids. In my opinion it neatly demonstrates the difficulties bioimage analysts face in every project:</p> <ul> <li>The volume is large (\\(1349 \\times 1505 \\times 1646\\) pixels) but not crazy large (fully fits into RAM on a decent machine but definitely not into GPU memory)</li> <li>The imaging method (FIB-SEM) is well-established but the sample preparation for this particular sample was quite special, so the visual appearance is unique</li> <li>Even if we imagine taking a model for cell boundaries trained on some other electron microscopy data (I'm not aware of the existance of any dataset that is even somewhat similar), it doesn't solve the problem of segmenting extracellular matrix</li> <li>Seems like <code>image == 0</code> should solve segmenting background class, but there are the lipid droplets which also have 0 signal. In principle we could apply an heuristic for segmenting background, like taking the largest connected component, but it is not transferable to other datasets (not shown here). One of the downstream tasks is counting lipid droplets per cell, so ignoring them is not an option as well  </li> <li>Cell boundaries can be super thin - down to 1 pixel wide - or very thick in different parts of the dataset, so it's not possible to downsample it further</li> <li>The dataset is quite unbalanced but the classes are distributed throughout the volume so it's not possible to pick just one foreground area and train on it</li> </ul> <pre><code>plt.figure()\nfor name, chan in channels.items():\n    plt.bar(chan, label[chan].sum() / label[chan].size, tick_label=full_names[name], color=cols[name])\n    print(name, label[chan].sum() / label[chan].size)\nnames = channels.keys()\nplt.xticks([channels[name] for name in names], [full_names[name] for name in names]);\nplt.ylabel(\"Proportion of the class is full dataset\")\n</code></pre> <pre><code>fg 0.564123369077732\nbg 0.2950981764296163\nboundaries 0.09271733890749893\nextra 0.04806111558515283\n\n\n\n\n\nText(0, 0.5, 'Proportion of the class is full dataset')\n</code></pre>  <p>As we can see, boundaries class takes up \\(9%\\) of the volume. Let's sample the training samples of size \\(128 \\times 128 \\times 128\\) from the dataset and check the distribution of the boundaries class.</p> <pre><code>from monai.data import GridPatchDataset, DataLoader, PatchIterd\npatch_iter = PatchIterd(keys=[\"image\", \"label\"], patch_size=(128, 128, 128), start_pos=(0, 0, 0))\ngrid_dataset = GridPatchDataset(data=ds, patch_iter=patch_iter)\nfrom tqdm import tqdm\nclass_distribution = {\"fg\": [], \"boundaries\": [], \"extra\": [], \"bg\": []}\n\nfor item in tqdm(DataLoader(grid_dataset, batch_size=1, num_workers=8)):\n    item = item[0]\n    img = item[\"image\"].numpy()[0]\n    lbl = item[\"label\"].numpy()[0].astype(np.uint16)\n    class_distribution[\"fg\"].append(lbl[0].sum() / lbl[0].size)\n    class_distribution[\"boundaries\"].append(lbl[1].sum() / lbl[1].size)\n    class_distribution[\"extra\"].append(lbl[2].sum() / lbl[2].size)\n    class_distribution[\"bg\"].append(lbl[3].sum() / lbl[3].size)\n</code></pre> <pre><code>1716it [01:03, 26.90it/s]\n</code></pre> <pre><code>plt.hist(class_distribution[\"boundaries\"], bins=100, color=cols[\"boundaries\"])\nplt.ylabel(\"Number of samples\")\nplt.xlabel(\"Proportion of boundary class in training patch\")\n</code></pre> <pre><code>Text(0.5, 0, 'Proportion of boundary class in training patch')\n</code></pre>  <p>Most of the samples have no boundary class and the rest of the samples are far from the dataset-wide \\(10%\\). Sampling randomly, we would need batches of approximately N samples to correctly estimate the boundary class proportion in most samples.</p> <pre><code>import seaborn as sns\nimport pandas as pd\n\nsampled_proportions = pd.DataFrame()\nfor batch_size in (1, 2, 4, 8, 16, 32, 64, 128, 256):\n    sampled_proportions[batch_size] = [np.sum(np.random.choice(class_distribution[\"boundaries\"], replace=True, size=batch_size)) / batch_size for _ in range(1000)]\n</code></pre> <pre><code>plt.figure(figsize=(12, 4))\nsns.stripplot(sampled_proportions, jitter=0.4, alpha=0.2)\nplt.xlabel(\"batch size\")\nplt.ylabel(\"Proportion of boundaries class in each batch\")\n</code></pre> <pre><code>Text(0, 0.5, 'Proportion of boundaries class in each batch')\n</code></pre>  <p>So, to stay reasonably close to volume-wise \\(9%\\) we would need a batch size of \\(32\\) or more, which is impossible with current GPU memory. Of course, there can be variation within the samples of one class too but this little experiment gives us an idea of how diverse the data is and how bad we are at sampling it.  </p>"},{"location":"train-eval-disparity/#batchnorm-refresher","title":"<code>BatchNorm</code> refresher","text":"<p>Neural networks consist of a series of filters with normalization and nonlinearity layers in between. Feature normalization helps to make convergence faster and more stable with respect to training parameters. In most cases it's probably possible to train a network with no normalization layers at all - however, the search for optimal learning rate schedule and other parameters might require much more effort.</p> <p>The general formula for the normalization operation with input \\(x\\), output \\(y\\) is:</p> <p>$$ y = \\frac{x-\\mu}{\\sqrt{\\sigma^2 + \\epsilon}}, $$ where \\(\\mu\\) and \\(\\sigma\\) are normalization parameters and \\(\\epsilon\\) is a small constant used for numerical stability. Parameters \\(\\mu\\) and \\(\\sigma\\) can be estimated directly from the input \\(\\mu = \\mathrm{E}[x]\\) and \\(\\sigma^2 = \\mathrm{Var}[x]\\), where average can be taken either over each sample independently or over the whole batch. Alternatively, global normalization parameters independent of the current input can be used. A common strategy is to estimate the parameters as a running average over multiple samples: \\(p_{new} = (1 - momentum) \\times p_{old} + momentum \\times p_{t}\\). The update speed is determined by the \\(momentum\\), set to \\(0.1\\) by default. </p> <p>During training \\(BatchNorm\\) uses statistics of the current batch and collects a running average. During inference it uses saved running average. Although running mean and variance are not learned in a sense that there's no gradient descent with respect to these parameters, they are estimated from the training data.</p> <p>An important assumption here is that the feature statistics of training batches remain stable and correspond to dataset-wide statistics. Let's look at what happens when it's not the case.</p>"},{"location":"train-eval-disparity/#real-mean-and-variance-of-features-vs-batch-statistics","title":"Real mean and variance of features vs batch statistics","text":"<p>I trained a 3D U-Net to segment the electron microscopy volume introduced earlier. This model uses \\(BatchNorm\\):</p> <pre><code>from monai.utils import set_determinism\nfrom monai.data import DataLoader, PatchDataset\nfrom monai.transforms import RandSpatialCropSamplesd\n\nimport torch\nfrom no_tiling.models import load_model\n\nfrom torchvision.models.feature_extraction import get_graph_node_names, create_feature_extractor\n</code></pre> <pre><code>model_path = \"data/experiments/organoids_boundaries/setup_01/model_epoch_0025.pth\"\n\n# Monai settings\nset_determinism(seed=0)\n\n# Set device\nif torch.cuda.is_available():\n    print(\"GPU is available\")\n    device = torch.device(6)\nelse:\n    print(\"GPU is not available\")\n    device = torch.device(\"cpu\")\n\n# Load model    \nmodel = load_model(model_path).to(device)\nmodel.eval()\nprint(model)\n</code></pre> <pre><code>GPU is available\nBasicUNet features: (32, 64, 128, 32).\nScaling factors: [[2, 2, 2], [2, 2, 2]]\nUNet(\n  (conv_0): TwoConv(\n    (conv_0): Convolution(\n      (conv): Conv3d(1, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n      (adn): ADN(\n        (N): InstanceNorm3d(32, eps=1e-05, momentum=0.01, affine=False, track_running_stats=True)\n        (D): Dropout3d(p=0.0, inplace=False)\n        (A): LeakyReLU(negative_slope=0.1, inplace=True)\n      )\n    )\n    (conv_1): Convolution(\n      (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n      (adn): ADN(\n        (N): InstanceNorm3d(32, eps=1e-05, momentum=0.01, affine=False, track_running_stats=True)\n        (D): Dropout3d(p=0.0, inplace=False)\n        (A): LeakyReLU(negative_slope=0.1, inplace=True)\n      )\n    )\n  )\n  (downs): ModuleList(\n    (0): Down(\n      (max_pooling): MaxPool3d(kernel_size=[2, 2, 2], stride=[2, 2, 2], padding=0, dilation=1, ceil_mode=False)\n      (convs): TwoConv(\n        (conv_0): Convolution(\n          (conv): Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n          (adn): ADN(\n            (N): InstanceNorm3d(64, eps=1e-05, momentum=0.01, affine=False, track_running_stats=True)\n            (D): Dropout3d(p=0.0, inplace=False)\n            (A): LeakyReLU(negative_slope=0.1, inplace=True)\n          )\n        )\n        (conv_1): Convolution(\n          (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n          (adn): ADN(\n            (N): InstanceNorm3d(64, eps=1e-05, momentum=0.01, affine=False, track_running_stats=True)\n            (D): Dropout3d(p=0.0, inplace=False)\n            (A): LeakyReLU(negative_slope=0.1, inplace=True)\n          )\n        )\n      )\n    )\n    (1): Down(\n      (max_pooling): MaxPool3d(kernel_size=[2, 2, 2], stride=[2, 2, 2], padding=0, dilation=1, ceil_mode=False)\n      (convs): TwoConv(\n        (conv_0): Convolution(\n          (conv): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n          (adn): ADN(\n            (N): InstanceNorm3d(128, eps=1e-05, momentum=0.01, affine=False, track_running_stats=True)\n            (D): Dropout3d(p=0.0, inplace=False)\n            (A): LeakyReLU(negative_slope=0.1, inplace=True)\n          )\n        )\n        (conv_1): Convolution(\n          (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n          (adn): ADN(\n            (N): InstanceNorm3d(128, eps=1e-05, momentum=0.01, affine=False, track_running_stats=True)\n            (D): Dropout3d(p=0.0, inplace=False)\n            (A): LeakyReLU(negative_slope=0.1, inplace=True)\n          )\n        )\n      )\n    )\n  )\n  (upcats): ModuleList(\n    (0): UpCat(\n      (upsample): UpSample(\n        (deconv): ConvTranspose3d(128, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n      )\n      (convs): TwoConv(\n        (conv_0): Convolution(\n          (conv): Conv3d(128, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n          (adn): ADN(\n            (N): InstanceNorm3d(64, eps=1e-05, momentum=0.01, affine=False, track_running_stats=True)\n            (D): Dropout3d(p=0.0, inplace=False)\n            (A): LeakyReLU(negative_slope=0.1, inplace=True)\n          )\n        )\n        (conv_1): Convolution(\n          (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n          (adn): ADN(\n            (N): InstanceNorm3d(64, eps=1e-05, momentum=0.01, affine=False, track_running_stats=True)\n            (D): Dropout3d(p=0.0, inplace=False)\n            (A): LeakyReLU(negative_slope=0.1, inplace=True)\n          )\n        )\n      )\n    )\n    (1): UpCat(\n      (upsample): UpSample(\n        (deconv): ConvTranspose3d(64, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n      )\n      (convs): TwoConv(\n        (conv_0): Convolution(\n          (conv): Conv3d(96, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n          (adn): ADN(\n            (N): InstanceNorm3d(32, eps=1e-05, momentum=0.01, affine=False, track_running_stats=True)\n            (D): Dropout3d(p=0.0, inplace=False)\n            (A): LeakyReLU(negative_slope=0.1, inplace=True)\n          )\n        )\n        (conv_1): Convolution(\n          (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n          (adn): ADN(\n            (N): InstanceNorm3d(32, eps=1e-05, momentum=0.01, affine=False, track_running_stats=True)\n            (D): Dropout3d(p=0.0, inplace=False)\n            (A): LeakyReLU(negative_slope=0.1, inplace=True)\n          )\n        )\n      )\n    )\n  )\n  (final_conv): Conv3d(32, 4, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n  (final_activation): Softmax(dim=1)\n)\n</code></pre> <p>The network starts with the convolution <code>conv_0</code> followed by a normalization layer and nonlinearity. In principle normalization layers are supposed to standardize the features: center the distribution around zero and force unit variance. But does it actually happen when the batch size is small?</p> <p>Let's extract the outputs of the very first convolution and look at their statistics.</p> <pre><code>nodes, _ = get_graph_node_names(model)\nprint(nodes)\n\nfeature_extractor = create_feature_extractor(model, return_nodes=['conv_0.conv_0.conv'])\n</code></pre> <pre><code>['x', 'conv_0.conv_0.conv', 'conv_0.conv_0.adn.N', 'conv_0.conv_0.adn.D', 'conv_0.conv_0.adn.A', 'conv_0.conv_1.conv', 'conv_0.conv_1.adn.N', 'conv_0.conv_1.adn.D', 'conv_0.conv_1.adn.A', 'downs.0.max_pooling', 'downs.0.convs.conv_0.conv', 'downs.0.convs.conv_0.adn.N', 'downs.0.convs.conv_0.adn.D', 'downs.0.convs.conv_0.adn.A', 'downs.0.convs.conv_1.conv', 'downs.0.convs.conv_1.adn.N', 'downs.0.convs.conv_1.adn.D', 'downs.0.convs.conv_1.adn.A', 'downs.1.max_pooling', 'downs.1.convs.conv_0.conv', 'downs.1.convs.conv_0.adn.N', 'downs.1.convs.conv_0.adn.D', 'downs.1.convs.conv_0.adn.A', 'downs.1.convs.conv_1.conv', 'downs.1.convs.conv_1.adn.N', 'downs.1.convs.conv_1.adn.D', 'downs.1.convs.conv_1.adn.A', 'upcats.0.upsample.deconv', 'upcats.0.convs.conv_0.conv', 'upcats.0.convs.conv_0.adn.N', 'upcats.0.convs.conv_0.adn.D', 'upcats.0.convs.conv_0.adn.A', 'upcats.0.convs.conv_1.conv', 'upcats.0.convs.conv_1.adn.N', 'upcats.0.convs.conv_1.adn.D', 'upcats.0.convs.conv_1.adn.A', 'upcats.1.upsample.deconv', 'upcats.1.convs.conv_0.conv', 'upcats.1.convs.conv_0.adn.N', 'upcats.1.convs.conv_0.adn.D', 'upcats.1.convs.conv_0.adn.A', 'upcats.1.convs.conv_1.conv', 'upcats.1.convs.conv_1.adn.N', 'upcats.1.convs.conv_1.adn.D', 'upcats.1.convs.conv_1.adn.A', 'final_conv', 'final_activation']\n</code></pre> <pre><code>running_mean = model.conv_0.conv_0.adn.N.running_mean.cpu().numpy()\nrunning_var = model.conv_0.conv_0.adn.N.running_var.cpu().numpy()\n</code></pre> <pre><code>n_samples = 100\n\ntrain_sampler = RandSpatialCropSamplesd(keys=[\"image\", \"label\"], roi_size=[128, 128, 128], num_samples=n_samples,\n                            random_center=True, random_size=False)\n\ntrain_patch_dataset = PatchDataset(data=ds,\n            patch_func=train_sampler,\n            samples_per_image=n_samples)\n\ntrain_loader = DataLoader(train_patch_dataset, batch_size=1)\n</code></pre> <pre><code>feature_extractor = create_feature_extractor(\nmodel, return_nodes=['conv_0.conv_0.conv'])\n\nconv_features = []\n\nfor tile in train_loader:\n    # Plot input\n    # print(tile[\"image\"])\n    # plt.imshow(tile[\"image\"][0, 0, 32, ...])\n    plt.show()\n    out = feature_extractor(tile[\"image\"].to(device))\n    conv_feat = out['conv_0.conv_0.conv'].detach().to(\"cpu\")\n    # Plot feature maps\n    # plt.imshow(conv_feat[0, 8, 32, ...])\n    # plt.show()\n    conv_features.append(conv_feat[0, :, ...])\n\ntile_mean = np.array([np.mean(feat, axis=(1, 2, 3)) for feat in conv_features])\ntile_var = np.array([np.var(feat, axis=(1, 2, 3)) for feat in conv_features])\n</code></pre> <pre><code>f, axes = plt.subplots(8, 4,  sharex=True, figsize=(15, 20))\n\nfor feat_idx, ax in enumerate(axes.flat):\n\n    ax.hlines(running_mean[feat_idx], xmin=0, xmax=n_samples, color=\"red\", label=\"running_mean\")\n    # plt.hlines(tile_mean[:, 0].mean(), xmin=0, xmax=100, color=\"green\")\n    ax.fill_between(range(n_samples), [running_mean[feat_idx] + running_var[feat_idx]] * n_samples,  [running_mean[feat_idx] - running_var[feat_idx]] * n_samples, color=\"pink\", alpha=0.8, label=\"running_var\")\n    ax.scatter(range(n_samples), tile_mean[:, feat_idx], marker=\".\", label=\"patch mean\")\n\n    ax.set_title(f\"feature map {feat_idx}\")\n\naxes[0, 0].legend()\nfor idx in range(4):\n    axes[7, idx].set_xlabel(\"sampled patches\")\nfor idx in range(8):\n    axes[idx, 0].set_ylabel(\"mean feature value\")\n\n\nplt.show()\n</code></pre>  <p>Here each of the 32 small panels corresponds to one feature map of the very first convolution layer. This convolution is applied directly to the input data and its output does not depend on the normalizations within the network. We just run the network on 100 random patches from the training subvolume using exactly the same data loader as was used while training the model. </p> <ul> <li>Each blue dot is a mean of the particular feature over one patch - this value is used by <code>BatchNorm</code> for normalization during training</li> <li>Red line is the <code>running_mean</code> remembered by <code>BatchNorm</code> during training - this value is used for normalization during inference</li> <li>Pink area is <code>running_var</code> remembered by <code>BatchNorm</code> during training</li> </ul> <p>With batch size 1 during training each patch gets nicely standardized because <code>BatchNorm</code> uses statistics of the input. During evaluation <code>running_mean</code> and <code>running_var</code> are used - and they are just wrong for most of the patches! </p> <p>But maybe it's not that bad? Let's check how this affects the predictions.</p>"},{"location":"train-eval-disparity/#traineval-disparity","title":"Train/eval disparity","text":"<p>Usually people assume that the network performs roughly the same in <code>train</code> and <code>eval</code> mode. But we have just seen that the feature maps are normalized completely differently. Let's try to run the network on the same patch in train and eval mode and see if there's a difference.</p> <pre><code>n_samples = 5\n\ntrain_sampler = RandSpatialCropSamplesd(keys=[\"image\", \"label\"], roi_size=[128, 128, 128], num_samples=n_samples,\n                            random_center=True, random_size=False)\n\ntrain_patch_dataset = PatchDataset(data=ds,\n            patch_func=train_sampler,\n            samples_per_image=n_samples)\n\ntrain_loader = DataLoader(train_patch_dataset, batch_size=1)\n</code></pre> <pre><code>for tile in train_loader:\n\n    # Only show examples with boundary class in the ground truth\n    if tile[\"label\"][0, 1, 32, ...].sum() &gt; 0:\n\n        f, axes = plt.subplots(1, 4,  sharex=True, figsize=(20, 4))\n\n        axes[0].imshow(tile[\"image\"][0, 0, 32, ...], cmap=\"Greys_r\")\n        axes[0].set_title(\"Input image\")\n        axes[1].imshow(tile[\"label\"][0, 1, 32, ...], cmap=\"Greys_r\")\n        axes[1].set_title(\"Boundaries ground truth\")\n\n        model.eval()\n        out_eval = model(tile[\"image\"].to(device)).detach().to(\"cpu\")\n\n        axes[2].imshow(out_eval[0, 1, 32, ...], cmap=\"Greys_r\")\n        axes[2].set_title(\"Boundaries prediction model.eval()\")\n\n        model.train()\n        out_train = model(tile[\"image\"].to(device)).detach().to(\"cpu\")\n\n        axes[3].imshow(out_train[0, 1, 32, ...], cmap=\"Greys_r\")\n        axes[3].set_title(\"Boundaries prediction model.train()\")\n\n        plt.show()\n</code></pre>      <p>Predictions in <code>train</code> and <code>eval</code> modes are actually very different and the predictions in training mode are much closer to the ground truth - which makes sense, because the network was trained with normalization layers always using the statistics of the input batch. In this case <code>batch_size=1</code> so the plots where we looked at the real feature mean and remembered value accurately show what happened. </p> <p>The training looked all normal with both train and eval loss going down. There was a gap between train and eval metric (Mean Dice 0.75 for train and 0.65 for eval) but it could in theory result from the overfitting. </p>"},{"location":"train-eval-disparity/#traineval-disparity-metric","title":"Train/eval disparity metric","text":"<p>Of course, the difference is random patch to patch so it's better to run sliding window inference for the full dataset in both modes and compare the results. </p> <p>To measure the difference quantitatively, I introduced the following metric:  $$ train/eval\\ disparity = 1 - Dice(P_{train}, P_{eval}),  $$  where \\(P_{train}\\) is the prediction done with  <code>model.train()</code> and \\(P_{eval}\\) - with <code>model.eval()</code>. If the predictions match perfectly, mismatch is 0 and if they are the exact opposites of each other, it's 1.</p> <p>For our example dataset, train/eval mismatch for the boundary channel is 0.48, which is actually surprisingly strong difference. At this point the loss calculated during training is basically not representative of the real model performance all due to batch statistics instability. </p>"},{"location":"train-eval-disparity/#practical-tips","title":"Practical tips","text":""},{"location":"train-eval-disparity/#how-to-check-if-statistics-instability-ruins-my-networks-performance","title":"How to check if statistics instability ruins my network's performance?","text":"<ul> <li>Run your prediction pipeline with <code>model.eval()</code> and <code>model.train()</code> on the same data and compare the results </li> <li>Compare results visually or quantify using train/eval disparity metric</li> <li>The difference can be larger or smaller in different parts of the dataset. For the best evaluation try to either run prediction in both modes on the whole evaluation dataset or at least make sure to get a representative sampling</li> <li>This notebook shows the results for semantic segmentation but the same applies to other tasks. For example, for classification Dice in the train/eval disparity metric can be replaced with accuracy, F1 score or your favorite metric</li> </ul>"},{"location":"train-eval-disparity/#i-have-traineval-mismatch-what-to-do","title":"I have train/eval mismatch - what to do?","text":"<p>If you established that the model performance degrades when you switch from <code>model.train()</code> to <code>model.eval()</code>, unfortunately, there's not much to do but to retrain the model. This problem appears because the training batches were too random and not representative of the overall dataset distribution.   </p> <p>If you are willing to retrain, there are multiple options:</p> <ul> <li> <p>Use larger batch size</p> <p>As we have seen in our experiment with different batch sizes, the larger the batch size, the more likely it is that the samples correctly represent the diversity of the dataset. Chances are you are already using the largest possible batch size but hey this is a great reason to get a cooler GPU.</p> </li> <li> <p>Downsample the data</p> <p>Old image analysis wisdom says: solve your problem at the minimal possible resolution. Downsampling the image makes effective patch size larger, meaning that the same amount of GPU memory can be used to sample more of the dataset. In my experience, downsampling microscopy data is rarely viable because the objects of interest (cell or nuclei boundaries, organelles, membranes) are already at the edge of what is possible to recognize at the available resolution. </p> </li> <li> <p>Sample only foreground patches or control the proportion of classes in a batch for classification</p> <p>Sampling only patches that contain foreground is an easy way to reduce variability of the patches and make patch feature distributions more stable. In tasks such as nuclei segmentation in light microscopy (bright objects on mostly dark background), this approach works perfectly. Of course, for electron microscopy it would not work: if I were to sample only the cell boundaries, there would be no patches coming from middle part of the cells. There are many features inside the cell that resemble the boundary, so if they were not present in the training patches, who knows what the network would predict there.</p> <p>I looked into a lot of publications which use U-Net for segmentation of microscopy or medical images and most of them had some kind of weighted patch sampling. I believe that it was a secret for making their training work (as opposed to whatever was claimed in the paper, such as a fancy loss or specialized augmentation). In the remaining cases it could be that the training data was cropped to contain only the region of interest in the first place.</p> </li> <li> <p><code>BatchRenorm</code></p> <p>If none of the above are possible, I suggest giving <code>Batch Renormalization</code> a try. <code>BatchRenorm</code> is a normalization layer that uses running mean and variance both during training and during inference. I managed to achieve quite good results with it both for electron and fluorescent microscopy images However, the training takes a bit longer.</p> </li> </ul>"},{"location":"train-eval-disparity/#adding-more-ground-truth-doesnt-solve-the-problem","title":"Adding more ground truth doesn't solve the problem","text":"<p>The usual explanation of the mismatch between <code>train</code> and <code>eval</code> mode is overfitting. In principle overfitting could be partially solved with adding more ground truth by achieving more representative training sample. Batch statistics instability prevents the network from effectively using the training data, so even performance on the training set is suboptimal. Wrong estimation of mean and variance of the feature maps prevents the network from making good predictions in eval mode even for the data from the training set. </p>"},{"location":"train-eval-disparity/#conclusion","title":"Conclusion","text":"<ul> <li> <p>Biological images often have high variability within one image and are much larger than GPU memory. Patches sampled in training can have completely different class distribution and even completely miss some classes due to general class imbalance and image content distribution.</p> </li> <li> <p>Especially for 3D data the batch size is very limited by the GPU memory. It is pretty normal to use batch size 1 or 2. Together with high diversity within images this causes batch statistics of feature maps to be estimated incorrectly. Running mean and variance do not correspond to the training mode batch statistics, leading to a puzzling low prediction quality when the model is applied to the new data. </p> </li> <li> <p>Of course, this tutorial shows an extreme case. The strength of the effect depends on the interplay between the data homogeneity, sampling strategy, patch size and batch size. Due to this a training pipeline that worked perfectly fine on one dataset can completely fail on another similar dataset - and it's really hard to say why unless we keep the normalization in mind. </p> </li> </ul> <p>Let's say we want to segment mitochondria in different datasets. In one case in the training image, mitochondria happened to be evenly distributed throughout the cell for biological reasons - then the batch statistics will be rather stable because it's likely that each patch hits an area with some mitochondria. In the other case large areas of the cell don't have mitochondria - the batch statistics might be way less stable and the model might perform much worse. It's very hard to debug because the images might seem similar in terms of resolution or contrast, why would the same training pipeline not work for both? </p> <ul> <li>Fortunately, it's very easy to check if train/eval disparity is a problem in a particular case. Just check if predictions in both modes on the same image are close enough. If there is a significant difference but you are not convinced, it's possible to check the feature maps and see directly if statistics are different.</li> </ul> <p>For examples of different modalities, architectures (Transformer-based networks suffer from this too!) and quantitative results check out my manuscript Tiling artifacts and trade-offs of feature normalization in the segmentation of large biological images.</p>"}]}